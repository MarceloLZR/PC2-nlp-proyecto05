# ğŸ§  Proyecto 5: Pre-entrenamiento Autodidacta y Fine-Tuning Eficiente

Este proyecto explora tÃ©cnicas modernas de pre-entrenamiento de lenguaje (MLM y NSP) y adaptaciÃ³n a tareas especÃ­ficas mediante estrategias eficientes como **Adapters** y **LoRA**, usando un modelo tipo BERT.

---

## ğŸ¯ Objetivos

- Implementar pre-entrenamiento con **Masked Language Modeling (MLM)** y **Next Sentence Prediction (NSP)**.
- Aplicar tÃ©cnicas de fine-tuning completas y eficientes:
  - Full fine-tuning.
  - Adapters (Houlsby et al., 2019).
  - LoRA (Hu et al., 2021).
- Comparar desempeÃ±o, eficiencia y uso de recursos.
- Analizar el trade-off entre nÃºmero de parÃ¡metros entrenados y precisiÃ³n.

---

## ğŸ“¦ Dataset

- **Pre-entrenamiento**: Wikipedia simplificado (~100M tokens).
- **Fine-tuning**: SST-2 (Stanford Sentiment Treebank v2), tarea de clasificaciÃ³n de sentimientos.

---
