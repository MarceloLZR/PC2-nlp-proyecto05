{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68aa094-8303-434f-abf0-46393c8084fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/fine_tune.py\n",
    "\n",
    "\"\"\"\n",
    "Fine-tuning de modelo BERT preentrenado para clasificaci贸n de sentimientos.\n",
    "Tarea: binaria (positivo = 1, negativo = 0) usando corpus Wikipedia simplificado.\n",
    "Comparaci贸n entre full fine-tuning y LoRA.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from time import time\n",
    "\n",
    "#  Directorios\n",
    "MODEL_NAME = \"models/bert_pretrained\"\n",
    "SAVE_FULL = \"models/bert_finetuned_full\"\n",
    "SAVE_LORA = \"models/bert_finetuned_lora\"\n",
    "LOG_DIR = \"logs/finetune\"\n",
    "os.makedirs(SAVE_FULL, exist_ok=True)\n",
    "os.makedirs(SAVE_LORA, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# 锔 Par谩metros\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "\n",
    "# И Dataset binario basado en palabras clave\n",
    "print(\"Cargando corpus...\")\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.simple\", split=\"train[:1%]\")\n",
    "\n",
    "def crear_dataset_clasificacion(ejemplos):\n",
    "    textos = ejemplos[\"text\"]\n",
    "    pares = []\n",
    "    for texto in textos:\n",
    "        texto_lower = texto.lower()\n",
    "        if any(p in texto_lower for p in [\"good\", \"excellent\", \"amazing\", \"love\", \"great\", \"happy\", \"positive\"]):\n",
    "            label = 1\n",
    "        elif any(n in texto_lower for n in [\"bad\", \"hate\", \"terrible\", \"awful\", \"worst\", \"sad\", \"negative\"]):\n",
    "            label = 0\n",
    "        else:\n",
    "            continue\n",
    "        pares.append({\"text\": texto, \"label\": label})\n",
    "    return pares\n",
    "\n",
    "pares = crear_dataset_clasificacion(dataset)\n",
    "dataset_clasificacion = Dataset.from_list(pares)\n",
    "\n",
    "#  Tokenizaci贸n\n",
    "print(\"Tokenizando...\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset_clasificacion.map(tokenize_function)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "#  Funci贸n para entrenamiento\n",
    "\n",
    "def entrenar(model, output_dir):\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        save_steps=500,\n",
    "        logging_steps=100,\n",
    "        logging_dir=LOG_DIR,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=collator,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    start = time()\n",
    "    trainer.train()\n",
    "    end = time()\n",
    "\n",
    "    print(f\"Entrenamiento completado en {end - start:.2f} segundos\")\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "\n",
    "#  Full Fine-tuning\n",
    "print(\"\\n Fine-tuning completo...\")\n",
    "model_full = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "entrenar(model_full, SAVE_FULL)\n",
    "\n",
    "#  Fine-tuning con LoRA\n",
    "print(\"\\n Fine-tuning con LoRA...\")\n",
    "model_lora = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model_lora = get_peft_model(model_lora, peft_config)\n",
    "model_lora.print_trainable_parameters()\n",
    "\n",
    "entrenar(model_lora, SAVE_LORA)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
