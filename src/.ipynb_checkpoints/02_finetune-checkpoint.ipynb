{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68aa094-8303-434f-abf0-46393c8084fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|| 36.7k/36.7k [00:00<00:00, 23.3MB/s]\n",
      "Downloading readme: 100%|| 16.0k/16.0k [00:00<00:00, 14.1MB/s]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|| 134M/134M [00:11<00:00, 11.6MB/s]\n",
      "Generating train split: 100%|| 205328/205328 [00:01<00:00, 165803.53 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizando...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 872/872 [00:03<00:00, 245.17 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Fine-tuning completo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/bert_pretrained and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='109' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [109/109 23:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.492400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento completado en 1414.46 segundos\n",
      "\n",
      " Fine-tuning con LoRA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/bert_pretrained and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 148,994 || all params: 109,632,772 || trainable%: 0.1359\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='109' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [109/109 17:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.562800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento completado en 1077.71 segundos\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fine-tuning de modelo BERT preentrenado para clasificaci贸n de sentimientos.\n",
    "Tarea: binaria (positivo = 1, negativo = 0) usando corpus Wikipedia simplificado.\n",
    "Comparaci贸n entre full fine-tuning y LoRA.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from time import time\n",
    "\n",
    "#  Directorios\n",
    "MODEL_NAME = \"models/bert_pretrained\"\n",
    "SAVE_FULL = \"models/bert_finetuned_full\"\n",
    "SAVE_LORA = \"models/bert_finetuned_lora\"\n",
    "LOG_DIR = \"logs/finetune\"\n",
    "os.makedirs(SAVE_FULL, exist_ok=True)\n",
    "os.makedirs(SAVE_LORA, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "#  Par谩metros\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "\n",
    "#  Cargar dataset binario\n",
    "print(\"Cargando corpus...\")\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.simple\", split=\"train[:1%]\")\n",
    "\n",
    "def crear_dataset_clasificacion(ejemplos):\n",
    "    textos = ejemplos[\"text\"]\n",
    "    pares = []\n",
    "    for texto in textos:\n",
    "        texto_lower = texto.lower()\n",
    "        if any(p in texto_lower for p in [\"good\", \"excellent\", \"amazing\", \"love\", \"great\", \"happy\", \"positive\"]):\n",
    "            label = 1\n",
    "        elif any(n in texto_lower for n in [\"bad\", \"hate\", \"terrible\", \"awful\", \"worst\", \"sad\", \"negative\"]):\n",
    "            label = 0\n",
    "        else:\n",
    "            continue\n",
    "        pares.append({\"text\": texto, \"label\": label})\n",
    "    return pares\n",
    "\n",
    "pares = crear_dataset_clasificacion(dataset)\n",
    "dataset_clasificacion = Dataset.from_list(pares)\n",
    "\n",
    "#  Tokenizaci贸n\n",
    "print(\"Tokenizando...\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset_clasificacion.map(tokenize_function)\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "#  Funci贸n com煤n de entrenamiento\n",
    "def entrenar(model, output_dir):\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        save_steps=500,\n",
    "        logging_steps=100,\n",
    "        logging_dir=LOG_DIR,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=collator,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    start = time()\n",
    "    trainer.train()\n",
    "    end = time()\n",
    "\n",
    "    print(f\"Entrenamiento completado en {end - start:.2f} segundos\")\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "#  Fine-tuning completo\n",
    "print(\"\\n Fine-tuning completo...\")\n",
    "model_full = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "entrenar(model_full, SAVE_FULL)\n",
    "\n",
    "#  Fine-tuning con LoRA\n",
    "print(\"\\n Fine-tuning con LoRA...\")\n",
    "base_model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model_lora = get_peft_model(base_model, peft_config)\n",
    "model_lora.print_trainable_parameters()\n",
    "\n",
    "# Asegurar modo entrenamiento\n",
    "model_lora.train()\n",
    "\n",
    "# Entrenar y guardar adaptadores PEFT\n",
    "entrenar(model_lora, SAVE_LORA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1486218-b094-4c65-a270-2a6a69fa42bd",
   "metadata": {},
   "source": [
    "# LoRA guarda solo los adaptadores, no el modelo completo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29f8e0db-e454-4ffc-bb36-8b51735a8e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/bert_pretrained and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Ruta a adaptadores entrenados\n",
    "LORA_DIR = \"models/bert_finetuned_lora\"\n",
    "\n",
    "# Cargar config de PEFT\n",
    "config = PeftConfig.from_pretrained(LORA_DIR)\n",
    "\n",
    "# Cargar base + adaptadores\n",
    "base_model = BertForSequenceClassification.from_pretrained(config.base_model_name_or_path)\n",
    "model_lora = PeftModel.from_pretrained(base_model, LORA_DIR)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(config.base_model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58ee5401-4825-4c7d-bee2-438f4f95b805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Verificando modelo LoRA guardado...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/bert_pretrained and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto: This is the worst book I've read!\n",
      "Predicci贸n: Negativo\n"
     ]
    }
   ],
   "source": [
    "#  Verificaci贸n post-entrenamiento: cargar adaptadores LoRA correctamente\n",
    "print(\"\\n Verificando modelo LoRA guardado...\")\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Ruta a adaptadores entrenados\n",
    "LORA_DIR = SAVE_LORA\n",
    "\n",
    "# Cargar config de PEFT\n",
    "config = PeftConfig.from_pretrained(LORA_DIR)\n",
    "\n",
    "# Cargar base + adaptadores\n",
    "base_model = BertForSequenceClassification.from_pretrained(config.base_model_name_or_path)\n",
    "model_lora = PeftModel.from_pretrained(base_model, LORA_DIR)\n",
    "\n",
    "# Tokenizer tambi茅n debe venir del modelo base\n",
    "tokenizer = BertTokenizerFast.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# EjempLITO  de prueba r谩pida\n",
    "sample = \"This is the worst book I've read!\"\n",
    "inputs = tokenizer(sample, return_tensors=\"pt\", truncation=True)\n",
    "outputs = model_lora(**inputs)\n",
    "pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "print(f\"Texto: {sample}\")\n",
    "print(f\"Predicci贸n: {'Positivo' if pred == 0 else 'Negativo'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de5e1399-998a-42b8-ac38-6ecac6547893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/bert_pretrained and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Comparaci贸n de predicciones:\n",
      "Texto                                                        Full       LoRA      \n",
      "-------------------------------------------------------------------------------------\n",
      "I love this movie, it was excellent!                         Negativo   Negativo  \n",
      "This is the worst experience Ive ever had.                  Negativo   Positivo  \n",
      "The book was good, but not amazing.                          Negativo   Positivo  \n",
      "I hate the food, it made me sick.                            Negativo   Positivo  \n",
      "What a bad day!                                              Negativo   Positivo  \n",
      "This made me really happy today.                             Negativo   Negativo  \n",
      "Absolutely terrible service, never again.                    Negativo   Positivo  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "#  Rutas\n",
    "FULL_DIR = \"models/bert_finetuned_full\"\n",
    "LORA_DIR = \"models/bert_finetuned_lora\"\n",
    "\n",
    "#  Cargar modelo Full Fine-Tuning\n",
    "model_full = BertForSequenceClassification.from_pretrained(FULL_DIR)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(FULL_DIR)\n",
    "model_full.eval()\n",
    "\n",
    "#  Cargar modelo LoRA Fine-Tuning\n",
    "config_lora = PeftConfig.from_pretrained(LORA_DIR)\n",
    "base_model = BertForSequenceClassification.from_pretrained(config_lora.base_model_name_or_path)\n",
    "model_lora = PeftModel.from_pretrained(base_model, LORA_DIR)\n",
    "model_lora.eval()\n",
    "\n",
    "#  Lista de textos de prueba\n",
    "textos = [\n",
    "    \"I love this movie, it was excellent!\",\n",
    "    \"This is the worst experience Ive ever had.\",\n",
    "    \"The book was good, but not amazing.\",\n",
    "    \"I hate the food, it made me sick.\",\n",
    "    \"What a bad day!\",\n",
    "    \"This made me really happy today.\",\n",
    "    \"Absolutely terrible service, never again.\",\n",
    "]\n",
    "\n",
    "#  Funci贸n de predicci贸n\n",
    "def predecir(model, tokenizer, texto):\n",
    "    inputs = tokenizer(texto, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return \"Positivo\" if pred == 0 else \"Negativo\"\n",
    "\n",
    "#  Mostrar resultados\n",
    "print(\"\\n Comparaci贸n de predicciones:\")\n",
    "print(f\"{'Texto':<60} {'Full':<10} {'LoRA':<10}\")\n",
    "print(\"-\" * 85)\n",
    "for texto in textos:\n",
    "    pred_full = predecir(model_full, tokenizer, texto)\n",
    "    pred_lora = predecir(model_lora, tokenizer, texto)\n",
    "    print(f\"{texto[:55]:<60} {pred_full:<10} {pred_lora:<10}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aceebf-39e3-460e-945e-4d215197c062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
