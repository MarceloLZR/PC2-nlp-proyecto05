{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68aa094-8303-434f-abf0-46393c8084fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████████████████████████████████████████████| 36.7k/36.7k [00:00<00:00, 23.3MB/s]\n",
      "Downloading readme: 100%|██████████████████████████████████████████████████████████| 16.0k/16.0k [00:00<00:00, 14.1MB/s]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████████████████████████████████████████████████████████| 134M/134M [00:11<00:00, 11.6MB/s]\n",
      "Generating train split: 100%|████████████████████████████████████████| 205328/205328 [00:01<00:00, 165803.53 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizando...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████| 872/872 [00:03<00:00, 245.17 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Fine-tuning completo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/bert_pretrained and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='109' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [109/109 23:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.492400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento completado en 1414.46 segundos\n",
      "\n",
      "🔸 Fine-tuning con LoRA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/bert_pretrained and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 148,994 || all params: 109,632,772 || trainable%: 0.1359\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='109' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [109/109 17:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.562800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento completado en 1077.71 segundos\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fine-tuning de modelo BERT preentrenado para clasificación de sentimientos.\n",
    "Tarea: binaria (positivo = 1, negativo = 0) usando corpus Wikipedia simplificado.\n",
    "Comparación entre full fine-tuning y LoRA.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from time import time\n",
    "\n",
    "#  Directorios\n",
    "MODEL_NAME = \"models/bert_pretrained\"\n",
    "SAVE_FULL = \"models/bert_finetuned_full\"\n",
    "SAVE_LORA = \"models/bert_finetuned_lora\"\n",
    "LOG_DIR = \"logs/finetune\"\n",
    "os.makedirs(SAVE_FULL, exist_ok=True)\n",
    "os.makedirs(SAVE_LORA, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "#  Parámetros\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "\n",
    "#  Cargar dataset binario\n",
    "print(\"Cargando corpus...\")\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.simple\", split=\"train[:1%]\")\n",
    "\n",
    "def crear_dataset_clasificacion(ejemplos):\n",
    "    textos = ejemplos[\"text\"]\n",
    "    pares = []\n",
    "    for texto in textos:\n",
    "        texto_lower = texto.lower()\n",
    "        if any(p in texto_lower for p in [\"good\", \"excellent\", \"amazing\", \"love\", \"great\", \"happy\", \"positive\"]):\n",
    "            label = 1\n",
    "        elif any(n in texto_lower for n in [\"bad\", \"hate\", \"terrible\", \"awful\", \"worst\", \"sad\", \"negative\"]):\n",
    "            label = 0\n",
    "        else:\n",
    "            continue\n",
    "        pares.append({\"text\": texto, \"label\": label})\n",
    "    return pares\n",
    "\n",
    "pares = crear_dataset_clasificacion(dataset)\n",
    "dataset_clasificacion = Dataset.from_list(pares)\n",
    "\n",
    "#  Tokenización\n",
    "print(\"Tokenizando...\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset_clasificacion.map(tokenize_function)\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "#  Función común de entrenamiento\n",
    "def entrenar(model, output_dir):\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        save_steps=500,\n",
    "        logging_steps=100,\n",
    "        logging_dir=LOG_DIR,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=collator,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    start = time()\n",
    "    trainer.train()\n",
    "    end = time()\n",
    "\n",
    "    print(f\"Entrenamiento completado en {end - start:.2f} segundos\")\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "#  Fine-tuning completo\n",
    "print(\"\\n Fine-tuning completo...\")\n",
    "model_full = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "entrenar(model_full, SAVE_FULL)\n",
    "\n",
    "#  Fine-tuning con LoRA\n",
    "print(\"\\n Fine-tuning con LoRA...\")\n",
    "base_model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model_lora = get_peft_model(base_model, peft_config)\n",
    "model_lora.print_trainable_parameters()\n",
    "\n",
    "# Asegurar modo entrenamiento\n",
    "model_lora.train()\n",
    "\n",
    "# Entrenar y guardar adaptadores PEFT\n",
    "entrenar(model_lora, SAVE_LORA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1486218-b094-4c65-a270-2a6a69fa42bd",
   "metadata": {},
   "source": [
    "# LoRA guarda solo los adaptadores, no el modelo completo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29f8e0db-e454-4ffc-bb36-8b51735a8e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/bert_pretrained and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Ruta a adaptadores entrenados\n",
    "LORA_DIR = \"models/bert_finetuned_lora\"\n",
    "\n",
    "# Cargar config de PEFT\n",
    "config = PeftConfig.from_pretrained(LORA_DIR)\n",
    "\n",
    "# Cargar base + adaptadores\n",
    "base_model = BertForSequenceClassification.from_pretrained(config.base_model_name_or_path)\n",
    "model_lora = PeftModel.from_pretrained(base_model, LORA_DIR)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(config.base_model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58ee5401-4825-4c7d-bee2-438f4f95b805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Verificando modelo LoRA guardado...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/bert_pretrained and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto: This is the worst book I've read!\n",
      "Predicción: Negativo\n"
     ]
    }
   ],
   "source": [
    "#  Verificación post-entrenamiento: cargar adaptadores LoRA correctamente\n",
    "print(\"\\n Verificando modelo LoRA guardado...\")\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Ruta a adaptadores entrenados\n",
    "LORA_DIR = SAVE_LORA\n",
    "\n",
    "# Cargar config de PEFT\n",
    "config = PeftConfig.from_pretrained(LORA_DIR)\n",
    "\n",
    "# Cargar base + adaptadores\n",
    "base_model = BertForSequenceClassification.from_pretrained(config.base_model_name_or_path)\n",
    "model_lora = PeftModel.from_pretrained(base_model, LORA_DIR)\n",
    "\n",
    "# Tokenizer también debe venir del modelo base\n",
    "tokenizer = BertTokenizerFast.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# EjempLITO  de prueba rápida\n",
    "sample = \"This is the worst book I've read!\"\n",
    "inputs = tokenizer(sample, return_tensors=\"pt\", truncation=True)\n",
    "outputs = model_lora(**inputs)\n",
    "pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "print(f\"Texto: {sample}\")\n",
    "print(f\"Predicción: {'Positivo' if pred == 0 else 'Negativo'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de5e1399-998a-42b8-ac38-6ecac6547893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/bert_pretrained and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Comparación de predicciones:\n",
      "Texto                                                        Full       LoRA      \n",
      "-------------------------------------------------------------------------------------\n",
      "I love this movie, it was excellent!                         Negativo   Negativo  \n",
      "This is the worst experience I’ve ever had.                  Negativo   Positivo  \n",
      "The book was good, but not amazing.                          Negativo   Positivo  \n",
      "I hate the food, it made me sick.                            Negativo   Positivo  \n",
      "What a bad day!                                              Negativo   Positivo  \n",
      "This made me really happy today.                             Negativo   Negativo  \n",
      "Absolutely terrible service, never again.                    Negativo   Positivo  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "#  Rutas\n",
    "FULL_DIR = \"models/bert_finetuned_full\"\n",
    "LORA_DIR = \"models/bert_finetuned_lora\"\n",
    "\n",
    "#  Cargar modelo Full Fine-Tuning\n",
    "model_full = BertForSequenceClassification.from_pretrained(FULL_DIR)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(FULL_DIR)\n",
    "model_full.eval()\n",
    "\n",
    "#  Cargar modelo LoRA Fine-Tuning\n",
    "config_lora = PeftConfig.from_pretrained(LORA_DIR)\n",
    "base_model = BertForSequenceClassification.from_pretrained(config_lora.base_model_name_or_path)\n",
    "model_lora = PeftModel.from_pretrained(base_model, LORA_DIR)\n",
    "model_lora.eval()\n",
    "\n",
    "#  Lista de textos de prueba\n",
    "textos = [\n",
    "    \"I love this movie, it was excellent!\",\n",
    "    \"This is the worst experience I’ve ever had.\",\n",
    "    \"The book was good, but not amazing.\",\n",
    "    \"I hate the food, it made me sick.\",\n",
    "    \"What a bad day!\",\n",
    "    \"This made me really happy today.\",\n",
    "    \"Absolutely terrible service, never again.\",\n",
    "]\n",
    "\n",
    "#  Función de predicción\n",
    "def predecir(model, tokenizer, texto):\n",
    "    inputs = tokenizer(texto, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return \"Positivo\" if pred == 0 else \"Negativo\"\n",
    "\n",
    "#  Mostrar resultados\n",
    "print(\"\\n Comparación de predicciones:\")\n",
    "print(f\"{'Texto':<60} {'Full':<10} {'LoRA':<10}\")\n",
    "print(\"-\" * 85)\n",
    "for texto in textos:\n",
    "    pred_full = predecir(model_full, tokenizer, texto)\n",
    "    pred_lora = predecir(model_lora, tokenizer, texto)\n",
    "    print(f\"{texto[:55]:<60} {pred_full:<10} {pred_lora:<10}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aceebf-39e3-460e-945e-4d215197c062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
