{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca5d7ca-e55b-45e2-8285-e201afc6f2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando corpus...\n",
      "Tokenizando...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████| 1026/1026 [00:03<00:00, 328.35 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../models/bert_pretrained and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [129/129 27:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.702300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ../models/bert_finetuned\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../models/bert_finetuned/tokenizer_config.json',\n",
       " '../models/bert_finetuned/special_tokens_map.json',\n",
       " '../models/bert_finetuned/vocab.txt',\n",
       " '../models/bert_finetuned/added_tokens.json',\n",
       " '../models/bert_finetuned/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notebooks/02_finetune_classification.ipynb\n",
    "\n",
    "# --------------------\n",
    "# Notebook: Fine-tuning de BERT pequeño para clasificación de texto\n",
    "# --------------------\n",
    "\n",
    "\"\"\"\n",
    "Este notebook ajusta finamente un modelo BERT para una tarea de clasificación de texto.\n",
    "Usamos el corpus de Wikipedia simplificada para generar ejemplos artificiales de clasificación binaria.\n",
    "\"\"\"\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import BertTokenizerFast, DataCollatorWithPadding\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "\n",
    "#  Parámetros\n",
    "MODEL_NAME = \"../models/bert_pretrained\"\n",
    "SAVE_DIR = \"../models/bert_finetuned\"\n",
    "BATCH_SIZE =8\n",
    "EPOCHS = 1\n",
    "\n",
    "#  Cargar corpus base\n",
    "print(\"Cargando corpus...\")\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.simple\", split=\"train[:1%]\")\n",
    "\n",
    "#  Crear un dataset de clasificación artificial (binaria)\n",
    "def crear_dataset_clasificacion(ejemplos):\n",
    "    textos = ejemplos[\"text\"]\n",
    "    pares = []\n",
    "    for i in range(0, len(textos) - 1, 2):\n",
    "        pares.append({\n",
    "            \"text\": textos[i],\n",
    "            \"label\": random.randint(0, 1)  # Etiqueta binaria aleatoria\n",
    "        })\n",
    "    return pares\n",
    "\n",
    "pares = crear_dataset_clasificacion(dataset)\n",
    "dataset_clasificacion = Dataset.from_list(pares)\n",
    "\n",
    "#  Tokenización\n",
    "print(\"Tokenizando...\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset_clasificacion.map(tokenize_function, batched=False)\n",
    "\n",
    "#  Collator\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 🧠 Modelo de clasificación\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "#  Entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=SAVE_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    logging_dir=\"../logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    acc = (predictions == torch.tensor(labels)).float().mean().item()\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Entrenando modelo...\")\n",
    "trainer.train()\n",
    "\n",
    "print(f\"Modelo guardado en {SAVE_DIR}\")\n",
    "model.save_pretrained(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "972ecd0a-0f5b-41fe-be03-5b4972a591e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Cargar modelo y tokenizer\n",
    "MODEL_DIR = \"../models/bert_finetuned\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_DIR)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "\n",
    "# Poner modelo en modo evaluación\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fa2a035-f46e-4a07-824f-d478e0a78c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir(texto):\n",
    "    # Tokenizar\n",
    "    inputs = tokenizer(texto, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    # Inferencia\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        pred = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "362df7ba-a190-4568-bdb6-4e38a415424a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción 1: 0\n",
      "Predicción 2: 0\n",
      "Predicción 3: 0\n"
     ]
    }
   ],
   "source": [
    "ejemplo1 = \"This article explains the history of the internet and its protocols.\"\n",
    "ejemplo2 = \"Random gibberish without meaning and very little coherence.\"\n",
    "ejemplo3 = \"The Earth revolves around the Sun and not the other way around.\"\n",
    "\n",
    "print(\"Predicción 1:\", predecir(ejemplo1))  # → 0 o 1\n",
    "print(\"Predicción 2:\", predecir(ejemplo2))  # → 0 o 1\n",
    "print(\"Predicción 3:\", predecir(ejemplo3))  # → 0 o 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40172c85-aaaa-4761-9ac1-fb8b00b40df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb3c35f-0f96-4994-ada9-388504fc3c92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
